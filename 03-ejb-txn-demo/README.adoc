= Demonstrates remote EJB calls and transaction propagation

== What is it?

Project demonstrates the remote EJB calls over two application servers.

[cols="40%,60%",options="headers"]
|===
|Project |Description

|`client`
|An application that needs to be deployed to the first server. It contains an EJB which calls
 the EJB application on the second server.
 For the EJB processing will be enlisted to the JTA transaction and processed with the two-phase
 commit there is an execution of JMS message call.

|`server`
|An application which is about tobe deployed to the second server. It contains an EJB which is capable
 to receive a remote call from the first server.
 For the EJB processing being enlisted to the JTA transaction there is a database insertion
 being part of the logic in the EJB business method. Then there is a special XAResource
 enlisted to the transaction for purposes of this quickstart demonstration.

|===

== Setup the environment

We need 3 instances of WildFly. Tested with WildFly 19.0.0.Beta1.

[code, bash]
----
# dowload the zip distribution from http://wildfly.org
# client server: client.war
unzip ~/Downloads/eap/wildfly-19.0.0.Beta1.zip; mv wildfly-19.0.0.Beta1/ wfly1
# server server: server.war
cp -r wfly1/ wfly2/
cp -r wfly1/ wfly3/
----

Configure the credentials for remote call authentication

[source,bash]
----
# go to the directory wfly2 and wfly3 and do the same
cd wfly2
# add user to 'ApplicationRealm' with command line tool
./bin/add-user.sh -a -u ejb -p ejb -ds
# with parameter -ds secret will be printed
## To represent the user add the following to the server-identities definition <secret value="ZWpi" />
----

Configure the client server with ejb security realm and remote outbound connection

[source,bash]
----
# 1. go to the directory with distribution of wfly1
cd wfly1
# 2. configure EJB remote endpoints
./bin/jboss-cli.sh --file=~/presentations/wildfly-kubernetes/03-ejb-txn-demo/client/extensions/local-ejb-configuration.cli
# 3. configure JMS queue
./bin/jboss-cli.sh --file=~/presentations/wildfly-kubernetes/03-ejb-txn-demo/client/extensions/local-jms-configuration.cli
----

Database is configured under `h2-ds.xml` descritor at `server`.

== Build, deploy and start WildFly

[source,sh]
----
# package and deploy
mvn clean package
cp ~/presentations/wildfly-kubernetes/03-ejb-txn-demo/client/target/client.war ~/tmp/wfly/wfly1/standalone/deployments
cp ~/presentations/wildfly-kubernetes/03-ejb-txn-demo/server/target/server.war ~/tmp/wfly/wfly2/standalone/deployments
cp ~/presentations/wildfly-kubernetes/03-ejb-txn-demo/server/target/server.war ~/tmp/wfly/wfly3/standalone/deployments

# start server (each one on its console)
cd ~/tmp/wfly/wfly1 && ./bin/standalone.sh -c standalone-full.xml  -Djboss.tx.node.id=wfly1 -Djboss.node.name=wfly1
cd ~/tmp/wfly/wfly2 && ./bin/standalone.sh -c standalone-ha.xml -Djboss.tx.node.id=wfly2 -Djboss.node.name=wfly2 -Djboss.socket.binding.port-offset=100
cd ~/tmp/wfly/wfly3 && ./bin/standalone.sh -c standalone-ha.xml -Djboss.tx.node.id=wfly3 -Djboss.node.name=wfly3 -Djboss.socket.binding.port-offset=200
----

== Test the HTTP invocations

[source,sh]
----
# transactional stateless
curl -s localhost:8080/client/stateless-tx | jq .
# non-transactional stateless
curl -s localhost:8080/client/stateless-notx | jq .
# non-transactional stateful
curl -s localhost:8080/client/stateful-notx | jq .

# verify on server
curl -s localhost:8180/server/users | jq '.[].id'
curl -s localhost:8280/server/users | jq '.[].id'
curl -s localhost:8180/server/commits

# crash
curl -X DELETE localhost:8180/server/users && printf ':' && curl -X DELETE localhost:8280/server/users
curl -s localhost:8080/client/fail

# after invocation restart the crashed server (either wfly2 or wfly3)
# check number of users
curl -s localhost:8180/server/users
# force recovery to happen now
telnet localhost 4712
>> SCAN
----

== Build for Kubernetes

[NOTE]
====
For `minikube` enable `registry` plugin and run `tunnel` for services to be exposed outside
of the Kubernetes cluster.

[source,sh]
----
minikube addons enable registry
# or eval docker environment to the shell directly: eval $(minikube docker-env)
minikube tunnel
----

To permit the localhost `docker` to push to the insecure minikube registry
edit (or create) `/etc/docker/daemon.json` and add there ip address with port
which you found when invoke `echo $(minikube ip):5000`.

Similar to
[code,yaml]
----
{
  "insecure-registries": ["192.168.39.132:5000"]
}
----
Restart docker
[code,sh]
----
systemctl daemon-reload; systemctl restart docker
----
====

[source,bash]
----
cd 03-ejb-txn-demo/
# build the project
mvn clean install

# building the parent pom and changing mvn command from package to install
# with install the built artifact will be part of the resulted local maven repository
s2i build -e MAVEN_S2I_GOALS=install . quay.io/wildfly/wildfly-centos7 demo-parent

# using pre-built image with updated local maven repository
# pointing to 'extensions' directory where cli scripts resides
s2i build \
  -e GALLEON_PROVISION_DEFAULT_FAT_SERVER=true -e S2I_IMAGE_SOURCE_MOUNTS=extensions \
  client/ demo-parent demo-client-deployment-build
s2i build \
  -e GALLEON_PROVISION_DEFAULT_FAT_SERVER=true -e S2I_IMAGE_SOURCE_MOUNTS=extensions \
  server/ demo-parent demo-server-deployment-build
----

With the build default servers, let's build the runtime servers

[source,sh]
----
cd s2i-runtime-image
# change Dockerfile `--from` to `demo-client-deployment-build`, enable JAVA_OPTS_APPEND
docker build --squash -t $(minikube ip):5000/demo/client-deployment . && \
docker push $(minikube ip):5000/demo/client-deployment
# change Dockerfile `--from` to `demo-server-deployment-build`, comment out JAVA_OPTS_APPEND
docker build --squash -t $(minikube ip):5000/demo/server-deployment . && \
docker push $(minikube ip):5000/demo/server-deployment
----

Now we can deploy

[source,sh]
----
kubectl create deployment client --image=localhost:5000/demo/client-deployment
# kubectl set env deployment/client STATEFULSET_HEADLESS_SERVICE_NAME=client
kubectl expose deployment client --type=LoadBalancer --port=8080

kubectl create deployment server --image=localhost:5000/demo/server-deployment
# kubectl set env deployment/server STATEFULSET_HEADLESS_SERVICE_NAME=server
kubectl expose deployment server --type=LoadBalancer --port=8080

kubectl scale deployment server --replicas=2
----

Now we want to scale up the server pods where we need to tune a bit the Kubernetes config

[source,sh]
----
# for clustering would work there is used KUBE_PING protocol currently
# the pod has to have rights to list all available pods under namespace. One way is to add 'view'
# role to the default service account
cat << EOF | kubectl create -f -
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: view
subjects:
- kind: ServiceAccount
  name: default
roleRef:
  kind: ClusterRole
  name: view
  apiGroup: rbac.authorization.k8s.io
EOF
----

Now executing the endpoints

[source,sh]
----
# check the external ip addresses for the cluster services
kube get svc
curl $(minikube service client --url)/client/stateless-tx | jq .
----

=== Issues

Bad practices in general for cloud

* data component should be separated from the business logic

Troubles of `Deployment`

* it does not preserve hostname which is problematic for recovery calls going to finish the transaction
* it does not preserve the persistence storage per pod where data folder is needed for storing transactional data
** it could be walked around with shared peristent storage mounted to all `Deployment`s of the one type

Here the WildFly Operator gives the helping hand

=== Fixing the issues of the "naive" transfer to Kubernetes

[NOTE]
====
Delete what was deployed previously
[source]
----
kube delete deployment service --all
----

====

. Deployment PostgreSQL database on Kubernetes
+
[source,bash]
----
kubectl create -f client/extensions/postgresql.deployment.yaml
----

. Deploy Artemis MQ on Kubernetes (???)
+
[source,bash]
----
# define role and account
kube create -f https://raw.githubusercontent.com/rh-messaging/activemq-artemis-operator/master/deploy/service_account.yaml
kube create -f https://raw.githubusercontent.com/rh-messaging/activemq-artemis-operator/master/deploy/role.yaml
kube create -f https://raw.githubusercontent.com/rh-messaging/activemq-artemis-operator/master/deploy/role_binding.yaml

# define CRDs
kubectl create -f https://raw.githubusercontent.com/rh-messaging/activemq-artemis-operator/master/deploy/crds/broker_v2alpha1_activemqartemisscaledown_crd.yaml
kubectl create -f https://raw.githubusercontent.com/rh-messaging/activemq-artemis-operator/master/deploy/crds/broker_v2alpha1_activemqartemisaddress_crd.yaml
kubectl create -f https://raw.githubusercontent.com/rh-messaging/activemq-artemis-operator/master/deploy/crds/broker_v2alpha1_activemqartemis_crd.yaml

# create the operator deployment
kubectl create -f https://raw.githubusercontent.com/rh-messaging/activemq-artemis-operator/master/deploy/operator.yaml

# upload the application definition as CustomerResource

----

Now we deploy with WildFly Operator

Do changes in source code

* rename `server/main/webapp/WEB-INF/h2-ds.xml` to `server/main/webapp/WEB-INF/h2-ds.xml.temp`
* check `client/extensions` files `postconfigure.sh` and `kubernetes-jms-configuration.cli` (to point with headless service)

[source,sh]
----
# build the image with galleon layers
s2i build -e GALLEON_PROVISION_DEFAULT_FAT_SERVER=true -e S2I_IMAGE_SOURCE_MOUNTS=extensions \
  client/ demo-parent demo-client-build
s2i build -e GALLEON_PROVISION_DEFAULT_FAT_SERVER=true -e S2I_IMAGE_SOURCE_MOUNTS=extensions \
  server/ demo-parent demo-server-build
# build the runtime image
cd s2i-runtime-image
# change Dockerfile `--from` to `demo-client-build`, enable JAVA_OPTS_APPEND
docker build --squash -t $(minikube ip):5000/demo/client .
docker push $(minikube ip):5000/demo/client
# change Dockerfile `--from` to `demo-server-build`, comment out JAVA_OPTS_APPEND
docker build --squash -t $(minikube ip):5000/demo/server .
docker push $(minikube ip):5000/demo/server
----

Now we have the images prepared. The images configured following

* `client/configuration`
** xml descriptor of `wildlfly-config-url` property
* `server/configuration`
** properties file `application-users.properties` that configures a user `ejb` to be authorized on receiving EJB calls
* `client/extensions/remote-configuration.cli`
** sockets, security realm and remote outbound connection for connecting to the `server` deployment
** enabling transaction manager socket to accept calls to execute transaction recovery
** http socket client mapping for https://github.com/wildfly/wildfly/blob/master/docs/src/main/asciidoc/_developer-guide/ejb3/EJB_on_Kubernetes.adoc#ejb-configuration-for-kubernetes[EJB remoting works]
* `client/extensions/clustering.cli`
** adding jgroups extension and subsystem configuration
** reconfiguration of Infinispan caches for being distributed
** http socket client mapping for EJB remoting works

Now we need to prepare for WildFly Operator. See
https://github.com/wildfly/wildfly-operator/blob/master/build/run-minikube.sh

[NOTE]
====
More user friendly way how to install the WildFly Operator is via
https://operatorhub.io
which is easy to go with OpenShift.
====

[source,sh]
----
cd $GOPATH/src/github.com/wildfly/wildfly-operator/
./src/github.com/wildfly/wildfly-operator/build/run-minikube.sh
----

Then create `CustomerResource`s of kind `WildFlyServer`.

[source,sh]
----
cd 03-ejb-txn-demo
mvn clean install
kubectl create -f ./client/client-cr.yaml
kubectl create -f ./server/server-cr.yaml
----

Now we can run

[source,sh]
----
curl -s $(minikube service client-loadbalancer --url)/client/stateless-tx | jq .
 url -s $(minikube service client-loadbalancer --url)/client/fail
----

[NOTE]
====
Way how to create own `xml` descriptor with config map
https://github.com/wildfly/wildfly-operator/tree/master/examples/clustering
====
